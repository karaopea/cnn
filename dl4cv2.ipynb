{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "AI Jump-up 교육은 실습위주 교육이라고 들어서 실습을 중간중간 실습할 수 있도록\n",
    "강의를 준비하게 되었고, 그래서 발표자료와 실습을 왔다갔다하는 문제를 방지하기 위해\n",
    "MD로 자료를 만들게 되었습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet, self).__init__()\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "def plot_loss_variation(result, **kwargs):\n",
    "\n",
    "    '''\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    '''\n",
    "    list_data = []\n",
    "    for epoch, train_loss in enumerate(result['train_losses']):\n",
    "        list_data.append({'type': 'train', 'loss': train_loss, 'epoch': epoch})\n",
    "    for epoch, val_loss in enumerate(result['val_losses']):\n",
    "        list_data.append({'type': 'val', 'loss': val_loss, 'epoch': epoch})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89)\n",
    "\n",
    "\n",
    "def plot_acc_variation(result, **kwargs):\n",
    "    list_data = []\n",
    "    for epoch, train_acc in enumerate(result['train_accs']):\n",
    "        list_data.append({'type': 'train', 'Acc': train_acc, 'test_acc': result['test_acc'], 'epoch': epoch})\n",
    "    for epoch, val_acc in enumerate(result['val_accs']):\n",
    "        list_data.append({'type': 'val', 'Acc': val_acc, 'test_acc': result['test_acc'], 'epoch': epoch})\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric, horizontalalignment='left', verticalalignment='center',\n",
    "                 transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)\n",
    "\n",
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
    "                                              batch_size=args.train_batch_size,\n",
    "                                              shuffle=True, num_workers=0)\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
    "                                            batch_size=args.test_batch_size,\n",
    "                                            shuffle=False, num_workers=0)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
    "                                             batch_size=args.test_batch_size,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    if args.model == 'CNN':\n",
    "        net = CNN()\n",
    "    elif args.model == 'Resnet':\n",
    "        net = Resnet()\n",
    "    else:\n",
    "        raise ValueError('In-valid model choice')\n",
    "    net.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(\n",
    "            'Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch,\n",
    "                                                                                                                  train_acc,\n",
    "                                                                                                                  val_acc,\n",
    "                                                                                                                  train_loss,\n",
    "                                                                                                                  val_loss,\n",
    "                                                                                                                  te - ts))\n",
    "\n",
    "    test_acc = test(net, partition, args)\n",
    "\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "print('ready.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. **Deep Learning for Computer Vision**\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/498_FA2019_lecture01.pdf_page_10.png?raw=true)\n",
    "### 3.1  Plan for this lecture\n",
    "1. d\n",
    "2. d\n",
    "3. d\n",
    "\n",
    "\n",
    "    이제 컴퓨터비젼에서 활용되는 딥러닝 기술들을 봅시다.\n",
    "    컴퓨터 비젼에 활용되는 현재 딥러닝 핵심 네트워크는 Convolutional Nerual Network입니다.\n",
    "    Covolutional Neural Network는 CNN, ConvNet이라고 대부분 줄여서 부르고 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Introduction\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/lecture_1_feifei.pdf_page_06.png?raw=true)\n",
    "\n",
    "    CNN의 등장은 컴퓨터비전의 다양한 부분에 영향을 줬습니다.\n",
    "    객체검출.......VQA(시각적 질의응답)까지 전체적인 정확도 향상을 가져왔습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 What is Convolutional Neural Network?\n",
    "#### 3.2.1 How human recognize an image?\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide27.PNG?raw=true)\n",
    "\n",
    "    Deep Learning을 강의에서 뉴런이 등장하듯이, CNN강의에서는 대부분 이 그림을 보실 수 있는데요,\n",
    "    과연 인간의 뇌가 MLP처럼 단순하게 동작할까? 좀 더 우리가 모르는 것이 있지 않을까?\n",
    "    사람이 이미지를 어떻게 인식하는지 생물학적(biologic)으로 알아보기 시작합니다.\n",
    "    그래서  저런 단순한 움직임을 보여주며, 고양이의 뇌를 관찰합니다.\n",
    "    선이 움직인다든지, 원이 커진다든지 이런 단순한 움직임을 보여줬는데.\n",
    "    이런 움직임에서는 이 뉴런들만 활성화되고, 이런 움직임에서는 이런 뉴런들만 활성화되는것을\n",
    "    발견할 수 있었습니다.\n",
    "    그래서 우리는 이미지를 볼때 이런 간단한 특징들을 담당하는 뉴런들이 있는게 아닐까 그렇게 생각을 하게 됩니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 What is Convolutional Neural Network?\n",
    "#### 3.2.1 How human recognize an image?\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide28.PNG?raw=true)\n",
    "\n",
    "    그리고 더 연구를 해보니까, 이 뉴런들이 계층 구조를 이루고 있다는 것을 알게됬거든요?\n",
    "    무작위하게 얽혀있는게 아니라, 시각세포와 연결되어 있는 이런 얕은(shallow)한 뉴런들이 있고\n",
    "    그리고 뒤에 이런 딥한 뉴런들이 있었는데, 이런 뉴런들이 언제 활성화가 되는지 살펴보니까\n",
    "    이런 얕은 얘들은 빛이 있냐 없냐, 어떻게 기울어져있냐 등 간단한 정보에 반응을하고\n",
    "    깊어지면 깊어질수록 이 선이 움직이느냐, 선이 이어져있느냐 끝나느냐.\n",
    "    깊어지면 깊어질수록 고차원적인 정보를 담당하는 것을 알아내게 됩니다.\n",
    "    그럼 이것을 어떻게 수학적으로 모델링해서 NN으로 만들 수 있을까? 이걸 고민해볼 차례겠죠.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FwFduRA_L6Q\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    얀 르쿤 교수님 팀에서 lenet이라는 convolutional network를 발표합니다.\n",
    "    hand-writting 문자를 성공적으로 인식하는 것을 보실 수 있습니다.\n",
    "    convolution, pooling등을 이용하며 이미지를 어떻게 딥러닝에 학습시킬지 방법론을 정립합니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.1 How can we feed images to a neural network?\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide31.PNG?raw=true)\n",
    "\n",
    "    그렇다면 이미지를 어떻게 뉴럴넷에 넣을 수 있을까요?\n",
    "    이런 이미지가 있다고 생각해봅시다.\n",
    "    배경에 나무라는 객체가 있는 심플한 이미지 입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.1 How can we feed images to a neural network?\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide32.PNG?raw=true)\n",
    "\n",
    "    이 이미지를 뉴럴넷에 입력으로 넣고 싶은데, 뉴럴넷은 입력으로 벡터를 받기 때문에\n",
    "    이미지는 2디멘션 그리드의 픽셀들입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.1 How can we feed images to a neural network?\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide34.PNG?raw=true)\n",
    "\n",
    "    이 뉴럴넷에 넣기 위해 이미지를 쭉펴서 벡터로 만듭니다.\n",
    "    이제 이 쫙 핀 이미지를 뉴럴넷에 넣으면 되겠네요.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.1 How can we feed images to a neural network?\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide35.PNG?raw=true)\n",
    "\n",
    "    하지만 기대와는 다르게 이렇게 넣으면 안됩니다.\n",
    "    첫번째 문제는 이미자안에서 나무가 조금만 이동하더라도 인풋에 대한 특징이 달라지게 되어\n",
    "    전혀다른 결과물을 만들게 됩니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.1 How can we feed images to a neural network?\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide36.PNG?raw=true)\n",
    "\n",
    "    우리는 이 이미지안에서 객체가 어느위치에 있든 같은 객체라고 인식하길 원합니다.\n",
    "    이것을 Translation invriance하다고 하며,\n",
    "    의미있는 정보를 위치에 상관없이 추출하고 같은 결과를 출력하길 원합니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.2 Convolution ≈  Cross-correlation\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide37.PNG?raw=true)\n",
    "\n",
    "    여기서 이런 객체의 위치에 구애받지 않고, 지역적 특징을 잘 추출하기 위해서\n",
    "    Convolution 연산을 사용합니다.\n",
    "    이해를 돕기 위해서 이미지를 1차원으로 줄였다고 가정하고 설명하겠습니다.\n",
    "    Convolution 연산의 수학적 의미는 왼쪽과 같고,\n",
    "    쉽게 커널(필터)를 뒤집고 곱해서 더한다라고 생각하시면됩니다.\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide38.PNG?raw=true)\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide39.PNG?raw=true)\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide40.PNG?raw=true)\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide41.PNG?raw=true)\n",
    "\n",
    "    인풋 시그널이 어디에 있든 result의 위치는 다르겠지만 같은 형태의 결과가 나올 것입니다.\n",
    "    이렇게 convolution연산과 softmax를 통해서 Translation invriance한 결과를 만듭니다.\n",
    "    실제 뉴럴넷에서는 커널을 뒤집는것이 중요하지 않기 때문에, 커널을 그냥 곱해주는\n",
    "    cross-correlation연산을 하지만 개념적인 부분만 사용한 것이기 때문에 Convolution이라고 부릅니다.\n",
    "    그래서 딥러닝에서는 아래 result를 가지고 우리가 원하는 정답값과 비교를 해가면서 차이를 줄이는 방향으로\n",
    "    위에 kernel형태를 학습을 반복하면서 변경해 나갑니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.2 Convolution operation\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/sobel.png?raw=true)\n",
    "\n",
    "    이건 sobel 필터라는 건데요 sobel이란 사람이 이미지의 외곽선 검출을 위해 kernel(필터)를 이렇게 정의를 한겁니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# haar like\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hPCTwxF0qf4\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    마찬가지로 이건 Viola, Jones가 정의한 haar-like feature인데요,\n",
    "    얼굴에 대한 특징을 여러개의 필터 이용해서 찾는 것을 보실 수 있습니다.\n",
    "    딥러닝에서는 어떻게 하면 목표 객체의 특징을 잘 찾을 수 있는 커널을 컴퓨터가 찾아주는 것이죠"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.2 Convolution operation\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide43.PNG?raw=true)\n",
    "\n",
    "    그럼 다시 2차원 이미지로 돌아와봅시다.\n",
    "    1차원때 설명한 것과 같은 방법으로 진행합니다.\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide44.PNG?raw=true)\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide45.PNG?raw=true)\n",
    "\n",
    "    한칸씩 이동하면서 곱하고 더해서 값을 만들고, 오른쪽 끝에 도달하면 한칸 아래로 내려와서\n",
    "    반복합니다. 이것을 이미지 끝까지 진행하면 오른쪽과 같은 2차원 결과가 만들어질 것입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.2 Convolution operation\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide46.PNG?raw=true)\n",
    "\n",
    "    우리는 3x3의 슬라이딩 윈도우를 커널이라고 부르고,\n",
    "    이 커널과 곱하고 합해져서 나온 값을 feature map이라고 부릅니다.\n",
    "    그리고 이 feature map을 보시면\n",
    "    왼쪽 위 한칸이 상위의 3x3의 정보가 들어온것이기 때문에,\n",
    "    이 칸의 Receptive filed는 여기 3x3이다 라고 말합니다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.2 Convolution operation\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide47.PNG?raw=true)\n",
    "\n",
    "    이 3x3 convolution unit들은 뉴럴넷의 뉴런과 같은 기능을 하고 있으며,\n",
    "    weight와 bias값을 가지고 이미지에서 선, 색상등 다양한 feature를 잘 추출하기 위해 학습되어 집니다.\n",
    "\n",
    "    그럼 이미지를 하나 불러서 convolution 연산을 실습해봅시다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select car image in cifar10 dataset\n",
    "img = partition['train'].dataset.data[4]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print('1.image shape:',img.shape)\n",
    "\n",
    "# trans np to torch (h,w,c) -> (c,h,w)\n",
    "x = transform(img)\n",
    "print('2.tensor shape:',x.shape)\n",
    "\n",
    "conv = nn.Conv2d(3, 3, 3, stride=1)\n",
    "# add batch dim\n",
    "x = x.unsqueeze(0)\n",
    "x = conv(x)\n",
    "imshow(x.squeeze().detach().cpu())\n",
    "print('3.shape of feature map after conv:',x.squeeze().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.2 Convolution operation\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide52.PNG?raw=true)\n",
    "\n",
    "    방금 실습에서는 3개의 커널을 사용해서 convolution연산을 했습니다.\n",
    "    커널이 많아질수록 각 필터가 예를들어 가로, 세로, 대각선 특징들을 뽑듯이\n",
    "    다양한 feature들을 추출할 수 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select car image in cifar10 dataset\n",
    "x = transform(img)\n",
    "print(x.shape)\n",
    "\n",
    "conv = nn.Conv2d(3, 3, 3, stride=1)\n",
    "x = x.unsqueeze(0)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.2 Convolution operation\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide55.PNG?raw=true)\n",
    "\n",
    "    위 코드 처럼 convolution연산은 convolution을 거듭할 수록 다음 feature맵이 작아지게 됩니다.\n",
    "    입력 이미지가 작을 경우에는 convolution 연산을 몇번 할 수 없겠죠?\n",
    "    마찬가지로 입력이 계속 작아지니 네트워크도 깊게 쌓을 수 없을 것 입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.3 Padding\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide57.PNG?raw=true)\n",
    "\n",
    "    그래서 convolution 연산을 할 때, 테두리에 여분을 추가해서 입력이 축소되는 것을 방지합니다.\n",
    "    이렇게 여분을 추가하는 것을 padding이라 부르고,\n",
    "    input과 output크기가 같게 나오도록 conv하는 것을 same convolution이라고 부릅니다.\n",
    "    3x3 커널일때는 한줄만 추가하면 되지만, 5x5 커널을 사용한다면 두줄을 추가해야 same conv가 되겠죠?\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select car image in cifar10 dataset\n",
    "x = transform(img)\n",
    "print(x.shape)\n",
    "\n",
    "conv = nn.Conv2d(3, 3, 3, stride=1, padding=1)\n",
    "x = x.unsqueeze(0)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.4 Pooling\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide59.PNG?raw=true)\n",
    "\n",
    "    반대로 입력 이미지가 너무 클 경우를 생각해봅시다.\n",
    "    입력이 크다는 것은 유지해야할 parameter가 많아진다는 것을 뜻하는 것이고,\n",
    "    이는 모델이 너무 커져 학습속도도 느려지고 같은 파라미터값들이 많아지는 현상이 일어날 수 있습니다.\n",
    "    그래서 우리는 어느정도 이 feature map크기에서 특징추출을 한것 같다라는 생각이들면\n",
    "    pooling을 통해서 입력을 줄여줍니다. 보통 계산하기 편하기 위해 입력을 반으로 줄입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select car image in cifar10 dataset\n",
    "x = transform(img)\n",
    "print(x.shape)\n",
    "\n",
    "conv = nn.Conv2d(3, 3, 3, stride=1, padding=1)\n",
    "x = x.unsqueeze(0)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)\n",
    "x = conv(x)\n",
    "print(x.squeeze().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.5 Sequence of convolutional layers\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide61.PNG?raw=true)\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide63.PNG?raw=true)\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide64.PNG?raw=true)\n",
    "    위와같이 CNN은 Convolution, Activation Fuction인 ReLU, Pooling등을 반복하면서 연산을 해나갑니다.\n",
    "    첫번째 Convolution Kernel들은 색상이나 선의 방향등 단순한 feature들을 학습하고,\n",
    "    Layer가 깊어질수록 더이상은 인간이 이해하지 못하는 고차원적인 feature들이 representation됩니다.RGB기 때문에..\n",
    "    그래도 깊이 있는 애들이 복잡한 것을 탐지한다고 예상할 순 있겠네요.\n",
    "\n",
    "    shallow한곳에서는 단순한 feature를 인식하고 deep한곳에서는 간단한 정보를 취합함으로써 좀 더 복잡한 정보를 탐지한다.\n",
    "    이런식으로 뇌가 동작한다고 연구가 되어왔는데 그것을 잘 모방했다고 볼 수 있겠네요.\n",
    "\n",
    "    이 그림은 언제 Layer5의 뉴런들이 Activation되는지 weight를 쌓아가면서 계산하여 Visulization을 한건데요,\n",
    "    Layer 5까지오면 꽃잎의 형태, 사람얼굴, 강아지의 얼굴등 고차원적인 정보를 다루는 것을 볼 수 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select car image in cifar10 dataset\n",
    "img = partition['train'].dataset.data[4]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# trans np to torch (h,w,c) -> (c,h,w)\n",
    "\n",
    "x = transform(img)\n",
    "\n",
    "conv = nn.Conv2d(3, 3, 3, stride=1, padding=1)\n",
    "relu = nn.ReLU()\n",
    "pooling = nn.MaxPool2d(2,2)\n",
    "# add batch dim\n",
    "x = x.unsqueeze(0)\n",
    "x = relu(conv(x))\n",
    "imshow(x.squeeze().detach().cpu())\n",
    "x = pooling(relu(conv(x)))\n",
    "imshow(x.squeeze().detach().cpu())\n",
    "x = relu(conv(x))\n",
    "imshow(x.squeeze().detach().cpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    실습을 간단히 해보자면, 여기서는 커널(필터)들이 어떻게 학습되어지는진 볼 순 없지만\n",
    "    Conv연산을 거칠수록 feature맵이 어떻게 변화되는지 살펴보도록 합시다.\n",
    "    위에 설명드린것처럼 Convoltion연산을 하고, Activation function을 통과하고 중간에 풀링을 해보겠습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Convolutional Neural Network\n",
    "#### 3.3.5 Sequence of convolutional layers\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide65.PNG?raw=true)\n",
    "\n",
    "    이것은 Convolution Net의 기본 구조를 나타낸건데요. 한번 봅시다.\n",
    "    아까 말씀드렸듯이, Conv, ReLU, Pool을 반복하고, 마지막에 FC가 있네요.\n",
    "    FC는 MLP와 같은 말입니다. 마지막에 나온 고차원적인 정보를 vectorize 시켜서\n",
    "    MLP에 넣어주고, 이게 어떤 객체인지 MLP와 했던것과 똑같이 분류문제를 풉니다.\n",
    "    마지막 feature맵은 더이상 위치정보가 남아있지 않은 객체의 feature의 정보이기 때문에\n",
    "    MLP로 넣을 수 있게 되는거구요.\n",
    "\n",
    "    그럼 위와 그림과 같은 ConvNet을 하나 구현해볼까요?\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1 )\n",
    "        self.conv1_2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1 )\n",
    "        self.conv2_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        #32->16->8->4 : 4*4*64\n",
    "        self.fc = nn.Linear(4*4*64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1_1(x))\n",
    "        x = self.relu(self.conv1_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2_1(x))\n",
    "        x = self.relu(self.conv2_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3_1(x))\n",
    "        x = self.relu(self.conv3_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "#models = ['CNN', 'Resnet']\n",
    "args.model = 'CNN'\n",
    "args.act = 'relu'\n",
    "args.l2 = 0.00001\n",
    "args.optim = 'SGD'  # 'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 1e-3\n",
    "args.epoch = 40\n",
    "\n",
    "args.train_batch_size = 128\n",
    "args.test_batch_size = 32\n",
    "\n",
    "\n",
    "print(args)\n",
    "setting, result = experiment(partition, deepcopy(args))\n",
    "plot_loss_variation(result)\n",
    "plot_acc_variation(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide74.PNG?raw=true)\n",
    "\n",
    "    ImageNet 데이터 베이스를 이용해서 1000개의 클래스에 대한 top-5 error 그래프입니다.\n",
    "    cifar에 비해서 이미지도 크고(227x227x3) 분류해야될 클래스도 많기 때문에 어렵겠죠? 데이터셋도 더 크구요.\n",
    "    이 network들이 어떠한 테크닉을 사용하였는지, case study를 통해 같이 살펴보겠습니다.\n",
    "***\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.2 AlexNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide71.PNG?raw=true)\n",
    "\n",
    "    CNN을 사용해서 Imagenet Challange에서 우승한 최초의 딥러닝 네트워크인 alexnet입니다.\n",
    "    논문에도 아키텍쳐 다이어그램이 위에 짤린것으로 나타내는 데, 정확히는 모르겠지만 의도적으로 한것같다.\n",
    "    이 아키텍처 다이그램이 보통적으로 보이지 않는 이유는 2gpu를 사용해서 큰 모델을 학습시키기 위해서,\n",
    "    two-stream network입니다.\n",
    "    학습하는데, 6일이 걸렸음, 지금은 몇분만에 학습가능\n",
    "    alexnet의 특징은 처음으로 CNN을 제대로 사용할 수 있다고 말씀드릴 수 있겠는데요,\n",
    "    첫번째는 vanishing Gradient 문제를  해결하기위해 ReLU를 사용했으며,\n",
    "    overfitting을 피하기 위해 dropout, weight decay를 사용했습니다.\n",
    "    weight decay는 특정한 weight값이 너무 크면 큰 만큼 값을 줄여줘서(부식시킨다) regularization해주는 겁니다.\n",
    "    (신경망이 범용성을 갖도록 처리하는 거니까 regularization은 일반화 정도라고 생각하시면됩니다 normalization, 정규화와는 다른의미입니다.)\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.2 AlexNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide72.PNG?raw=true)\n",
    "\n",
    "    alexnet을 단순화 시켜보면 이렇습니다. input을 conv, non-linaer, pooing 반복반복,\n",
    "    마지막에 fcfcfc, lenet, cnn의 기본구조와는 거의 차이가없죠? 그래서 이 친구 같은 경우는 아직 advanced cnn은 아니다.\n",
    "    처음으로 convolution layer를 dnn에 잘 적용했기때문에 다뤄보았구요,\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/Lec6-A.pdf_page_17.png?raw=true)\n",
    "\n",
    "    이제부터 중요해지는 그 다음것을 봅시다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide75.PNG?raw=true)\n",
    "\n",
    "    VGGNet은 2014년도에 2위를 한 oxford대학의 네트워크입니다. 1위는 GoogLeNet이 했죠.\n",
    "    이 네트워크를 다루는 이유는 VGGNet의 네트워크 디자인 룰이 간단하고 효과적이기 때문에 아직까지도 많은 영향을 미치고 있기 때문입니다.\n",
    "    네트워크를 간단히 살펴보자면, conv,conv,pooling -> .... -> conv,conv,conv,conv,poopling,fcfcfc 구조네요.\n",
    "    conv를 연속적으로 한다는 것 빼고는 딱히 alexnet과 큰 차이는 없지만, 우리는 Design rule을 볼 필요가 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/498_FA2019_lecture08.pdf_page_041.png?raw=true)\n",
    "\n",
    "    첫번째 디자인 룰은 same convolution을 합니다. 위에서 한번 말씀드렸듯이 padding 1을 추가해서 feature map이 작아지는 것을\n",
    "    방지하여 좀 더 깊은 네트워크를 만들 수 있게 합니다. 그리고 conv-pooling을 하는 이전과는 다르게 conv-conv-pooling을 하는\n",
    "    구조를 보실 수 있으실텐데요, 우리는 가로세로 5칸의 window를 featuremap에 담고싶다고 하면,\n",
    "    (이 윈도우를 receptive field라 위에서 말씀드렸죠?) 5x5 conv를 한번 하는것보다,\n",
    "    3x3을 두번 하는 것이 효과적이다라는 것을 찾아내었기 때문인데요, 아래보시면\n",
    "    파라미터 수와 실수연산은 더 적은데 같은효과 오히려 non-linearity를 두 번 적용한 더 좋은 결과가 나온다는 것을 보실 수 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide76.PNG?raw=true)\n",
    "    두번째, 첫번째 레이어의 5x5 receptive filed를 보는 것과 같은 효과라는 것을 도식화 한 그림입니다.\n",
    "    왜냐하면 첫번째 conv-layer를 통과한 fateure-map의 한 칸이 input 3x3 정보를 담고 있기 때문이죠.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/498_FA2019_lecture08.pdf_page_044.png?raw=true)\n",
    "\n",
    "    두번째 룰은 plooing을 할때, feature map을 반으로 줄이고 채널 수를 두배로 키워줍니다.\n",
    "    이 룰은 아주 많은 cnn 아키텍쳐들이 따르고 있는 룰인데요,\n",
    "    각 conv스테이지마다 메모리는 줄이고 실수 연산cost는 유지하고 싶었기 때문입니다.\n",
    "    (Memory=H*W*C , Params=c_in*c_out*k_h*k_w, Params=(k_h*k_w*k_z)*filters_num, FLOPs=(c_out*h*w)*(c_in*k_h*k_w)\n",
    "***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide77.PNG?raw=true)\n",
    "\n",
    "    이 그래프는 VGG의 imagenet top-5 error인데요, 16레이어까지는 깊게 깧을 수록 에러가 낮아지는 것을 볼 수 있는데,\n",
    "    더 깊게 쌓으면 에러가 차라리 높아지는 것을 볼 수 있습니다.\n",
    "    깊어질수록 많아지는 연산량과 weight paramter들을 optimize를 하는 것이 어려움을 어떠한 네트워크 디자인을\n",
    "    통해 극복했는지 살펴보도록 합시다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: Inception module\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide92.PNG?raw=true)\n",
    "\n",
    "    14년도 Imagenet challange(ILSVRC) 1위를 한 GooLeNet입니다. 2위는 VGG구요.\n",
    "    GooLeNet에서 중간에 L자가 대문자인 이유는 얀르쿤교수팀이 개발한 Lenet을 shoutout하는 의미로 하였구요.\n",
    "    네트워크 아키텍쳐가 조금 복잡해보이는데요, 가장 눈에 띄는점은 뭐 이렇게 묶음이 있구요, 중간중간 곁가지 처럼 나온게 있네요.\n",
    "    이 묶음이랑 곁가지를 제외하고 본다면 일반적인 CNN구조와 유사하게 매우 단순한 구조로 가지고 있습니다.\n",
    "\n",
    "    우선 옆으로 나온 곁가지에 대해 설명드리자면, 끝에서 나온 loss는 초반부 layer에 영향이 약해지는 vanishing gardient문제를\n",
    "    조금이라도 보완하고자, 중간중간 loss도 뽑아서 역전파를 시키자는 trick입니다.\n",
    "    15년도에 batch normalizaion이라는 학습 방법이 나온 뒤는 더이상 사용하지 않습니다.\n",
    "\n",
    "    그럼 이 묶음은 뭘까요? 이 묶음에 대해서 자세히 알아보도록 합시다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: Inception module\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide93.PNG?raw=true)\n",
    "\n",
    "    이 묶음의 이름은 inception module이라고 합니다.\n",
    "    영화 인셉션에서 \"우리는 좀더 깊이 들어가야해.\"라는 대사가 나옵니다. 영화에선 꿈의 심연을 뜻하는거겠죠?\n",
    "    딥러닝에서도 좀 더 깊이 네트워크를 쌓고자하는 바램에서 inception이란 이름을 따왔습니다.\n",
    "\n",
    "    inception module을 크게 확하면 왼쪽과 같은데, parallel한 구조를 가지고 있습니다.\n",
    "    1x1 conv를 빼고보자면, 그냥 input도 넘겨주고,\n",
    "    3x3 conv한 것도 넘겨주고, 5x5 conv한것도 넘겨주고, 3x3 pooling 한것도 넘겨주네요,\n",
    "    한가지 input에 대해서 동시에 다양한 방면으로 본다는 의미에서 좋아 보입니다. 그리고 마지막에 합치는 것을 볼 수 있네요\n",
    "    여기서 합치는 것을 concatenate, stack 이라고 부르는데 값을 합치는 것이 아닌, 단순히 채널들을 순서대로 쌓는다는 의미입니다.\n",
    "\n",
    "    1x1 conv는 이렇게 합치다보면 너무 channel이 두꺼워지니까 1x1 conv를 통해 줄이는 연산입니다.\n",
    "    넓이와 높이를 줄이는데 pooling이 있듯이 channel을 줄이는데는 1x1 conv를 사용할 수 있으며\n",
    "    이 레이어를 병목처럼 줄인다고하여 bottleneck layer라고 합니다.\n",
    "\n",
    "    좀 더 쉽게 설명하자면 식빵이 들어오면, 이 식빵을 4개로 잘라서 다양한 4명의 사람이 살펴보고 특이한 것을 마킹을하고\n",
    "    다시 식빵을 합치는 겁니다. 근데 식빵이 이 과정을 할때마다 두꺼워져서 양옆에서 손으로 한번씩 눌러줘서 원래크기 비슷하게 맞춰줍니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/covariate_shift.jpg?raw=true)\n",
    "\n",
    "    그리고 15년도에 batch norm이 나온 후 부터는 곁가지 트릭은 쓰지 않는다고 말씀드렸습니다.\n",
    "    batch normalization, batch norm/bn이라고 부르는 이 방법은.\n",
    "    학습은 mini-batch단위로 진행하기 때문에 mini-batch의 평균과 분산을 구해서, 데이터를 -1~1사이로 normalization을 시켜줍니다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide80.PNG?raw=true)\n",
    "\n",
    "    그리고 15년도에 batch norm이 나온 후 부터는 곁가지 트릭은 쓰지 않는다고 말씀드렸습니다.\n",
    "    batch normalization, batch norm/bn이라고 부르는 이 방법은.\n",
    "    학습은 mini-batch단위로 진행하기 때문에 mini-batch의 평균과 분산을 구해서, 데이터를 -1~1사이로 normalization을 시켜줍니다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide81.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide82.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet: residual connections\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide83.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide84.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.7 DenseNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide86.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.7 Sqeeze-and-excitation networks\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide87.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.7 State-of-the-art\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide89.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.7 State-of-the-art\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide89.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.1 LeNet-5\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide70.PNG?raw=true)\n",
    "\n",
    "    필기체 문자를 인식하는 네트워크며, 이를위한 MNIST (28x28) 데이터셋이 마련되어있습니다.\n",
    "    CNN 입문할때 많이 실습하는 네트워크라, 실습도 해보신 분들이 많으실 것 같습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.2 AlexNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide71.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/Lec6-A.pdf_page_17.png?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide75.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide76.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide77.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.4 GoogLeNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide77.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 Chanllenges of depth\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide78.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: Inception module\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide79.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide79.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide80.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide81.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide82.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet: residual connections\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide83.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide84.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.7 DenseNet\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide86.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.7 Sqeeze-and-excitation networks\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide87.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.7 State-of-the-art\n",
    "![pt](https://github.com/karaopea/cnn/blob/master/images/pt/Slide89.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}